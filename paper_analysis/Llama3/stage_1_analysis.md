Based on the paper describing the Llama 3 Herd of language models, here are the key insights, surprising elements, and relevant quotes:

Key Insights:
1. Llama 3 delivers comparable quality to leading language models like GPT-4 across a wide range of tasks, despite being an open-source model. 
2. Scaling up data quality, model size, and compute significantly improved Llama 3's performance over previous versions.
3. Managing complexity through architectural choices like dense Transformers enabled stable scaling to very large model sizes.
4. Post-training techniques like supervised fine-tuning and preference optimization enhanced Llama 3's capabilities and alignment.
5. Llama 3 exhibits a better balance between helpfulness and safety compared to previous models.
6. Multimodal extensions to Llama 3 for images, video and speech are being developed but not yet released.
7. The public release of the flagship 405B Llama 3 model aims to spur research and responsible development of AGI.

Surprising Elements:
- The 405B parameter Llama 3 model matches or exceeds GPT-4's performance on many benchmarks, which is unexpected for an open-source model.
- Scaling up to 405B parameters and 15T of training data provided significant quality gains, contrasting beliefs about diminishing returns at very large scales.
- The focus on dense Transformers rather than more complex architectures like Mixture-of-Experts enabled stable scaling.

Relevant Quotes:
"Our experimental evaluation suggests that our flagship model performs on par with leading language models such as GPT-4 across a variety of tasks, and is close to matching the state-of-the-art."

"We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2."

"We pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens."

"We hope that the open release of a flagship model will spur a wave of innovation in the research community, and accelerate a responsible path towards the development of artificial general intelligence (AGI)."