Here's a summary capturing the style and perspectives of Swyx (Shawn Wang):

We're at an inflection point for AI evaluation, my friends. The traditional benchmarks just ain't cutting it when it comes to aligning these large language models with human preferences. It's like we've been playing a whole different game without realizing the goal posts moved. 

But don't worry, the dev community is on it. Researchers at Berkeley, Stanford, and other top institutions have been cooking up some fresh new benchmarks to really stress test these AI assistants. MT-Bench and Chatbot Arena put models through their paces on open-ended, multi-turn conversations that get to the heart of what users actually care about.

And here's where it gets really wild - they're using super-powered AI models like GPT-4 as judges to automatically evaluate performance. Kinda meta, right? But the results are promising, with the AI judges showing over 80% agreement with human raters. That's a big deal if we want scalable, cost-effective evaluation.

Of course, there are some potential pitfalls to watch out for. The AI judges can exhibit biases like preferring lengthier responses. And we can't treat them as a perfect proxy for human judgment. But by being upfront about the limitations while continuously improving the process, we're taking an important step.

This work opens up all kinds of possibilities for better aligning AI systems with what users and developers actually want. It's a great example of the ecosystem coming together - connecting threads across academia, open source, and the broader tech community. We're iterating in public and learning together.

For any devs or researchers watching, now is the time to dig in. Study the benchmarks, poke at the AI judge approach, and figure out how to make your models shine on these new tasks. The next phase of helpful, truthful, and scalable AI assistants is just getting started.