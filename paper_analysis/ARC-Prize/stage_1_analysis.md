Based on the technical report provided, here are the key insights, surprising elements, and notable quotes:

Key Insights:

1. ARC-AGI remains an extremely challenging benchmark for measuring general artificial intelligence, as the state-of-the-art score only increased from 33% to 55.5% after a global competition with substantial prizes.

2. The rise of large language models and scaling up of deep learning systems from 2020 to 2024 did not lead to significant progress on ARC-AGI, suggesting limitations in the ability of these approaches to tackle novel tasks without training data.

3. New techniques like deep learning-guided program synthesis and test-time training enabled some progress, but a substantial gap remains in achieving human-level performance on the benchmark.

4. The ARC-AGI benchmark challenges conventional thinking by focusing on generalization to novel tasks, rather than performance on tasks with available training data.

5. The competition highlights the importance of open sharing of ideas and techniques to drive progress towards artificial general intelligence.

Surprising or Shocking Elements:

1. Despite the widespread adoption of large language models and scaling up of deep learning systems, these approaches performed poorly on ARC-AGI, with the original GPT-3 model scoring 0% on the public evaluation set.

2. The top score of 55.5% on the private evaluation set was achieved by a team (MindsAI) that chose not to open-source their solution, potentially hindering progress in the field.

3. Even after multiple competitions and substantial prize incentives, the state-of-the-art score on ARC-AGI remains far below human-level performance (97-98% on the original private evaluation tasks).

Notable Quotes:

1. "ARC-AGI has resisted the rise of LLMs in the 2022-2024 period."

2. "Our view is that progress towards AGI had stalled during this period â€“ AI systems had been getting bigger and memorizing ever more training data, but generality in frontier AI systems had been roughly static."

3. "ARC-AGI has been the target of three public competitions before ARC Prize 2024... The first ARC-AGI competition ran on Kaggle in 2020 (9) with a top score of 20%. Four years later, the top score had only increased to 33%."

4. "The defining characteristic of the benchmark is that it should not be possible to prepare for any of the tasks in advance. Every task in the dataset follows a different logic."

5. "ARC Prize incentivizes and promotes open sharing."